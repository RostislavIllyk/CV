{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0c7cb3",
   "metadata": {},
   "source": [
    "# Постановка задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a76967",
   "metadata": {},
   "source": [
    "Нужно написать приложение, которое будет считывать и выводить кадры с веб-\n",
    "камеры. В процессе считывания определять что перед камерой находится человек,\n",
    "задетектировав его лицо на кадре. После этого, человек показывает жесты руками, а\n",
    "алгоритм должен считать их и определенным образом реагировать на эти жесты.\n",
    "На то, как система будет реагировать на определенные жесты - выбор за вами.\n",
    "Например, на определенный жест (жест пис), система будет здороваться с человеком.\n",
    "На другой, будет делать скриншот экрана. И т.д.\n",
    "Для распознавания жестов, вам надо будет скачать датасет для жестов рук."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58426a93",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ad5bd",
   "metadata": {},
   "source": [
    "Для решения задачи (после долгих раздумий связанных с выбором датасета) было принято решение самому создать мини-датасет с использованием веб-камеры с которой в дальнейшем и должна будет работать программа. Данные собирались (и размечались) автоматически с использованием - __mediapipe__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741434f",
   "metadata": {},
   "source": [
    "В данном ноутбуке программа обучается на этих данных и в режиме онлайн выводит название жеста а так же ведет журнал жестов. При появлении жеста __\"Ok\"__ происходит запись картинки на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1224f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "\n",
    "import glob \n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "from facenet_pytorch import MTCNN\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc393672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((937, 64), (576, 64), (540, 64), (2053, 64))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name_V = './video/data__0' \n",
    "file_name_O = './video/data__1'\n",
    "file_name_W = './video/data__2'\n",
    "\n",
    "LABELS = ['Victory', 'Ok', 'WTF']\n",
    "\n",
    "data_V = load_obj(file_name_V)\n",
    "data_O = load_obj(file_name_O)\n",
    "data_W = load_obj(file_name_W)\n",
    "\n",
    "data = np.vstack((data_V, data_O))\n",
    "data = np.vstack((data, data_W))\n",
    "\n",
    "\n",
    "perm = permutation(len(data))\n",
    "data = data[perm]\n",
    "\n",
    "data_V.shape, data_O.shape, data_W.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870fbd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2053, 63), (2053,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = data[:,:-1]\n",
    "y_data = data[:,-1].astype('int64')\n",
    "X_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d477817e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1539"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep = int(len(X_data)*0.75)\n",
    "sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c66022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "#     print(output.topk(5))\n",
    "    return LABELS[category_i], category_i\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18aa7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skeleton_Dataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "       \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.data[idx]        \n",
    "        label = self.labels[idx]        \n",
    "\n",
    "        return (item, label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb3a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Skeleton_Dataset(X_data, y_data)\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [sep, len(X_data)-sep])\n",
    "train_loader = DataLoader(train_dataset, batch_size = 256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "badf3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "      \n",
    "        self.output_dim = output_dim\n",
    "        self.fc1 = torch.nn.Linear(input_dim,256)\n",
    "        self.elu1 = torch.nn.ELU(inplace=True)\n",
    "        self.fc2 = torch.nn.Linear(256,512)        \n",
    "        self.elu2 = torch.nn.ELU(inplace=True)        \n",
    "        self.dr1 = torch.nn.Dropout(0.5)\n",
    "        self.fc3 = torch.nn.Linear(512,output_dim)\n",
    "        self.sm = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.fc1(x)\n",
    "        x = self.elu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.elu2(x)        \n",
    "        x = self.dr1(x)    \n",
    "        x = self.fc3(x)        \n",
    "        out = self.sm(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "758d2bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_net(\n",
       "  (fc1): Linear(in_features=63, out_features=256, bias=True)\n",
       "  (elu1): ELU(alpha=1.0, inplace=True)\n",
       "  (fc2): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (elu2): ELU(alpha=1.0, inplace=True)\n",
       "  (dr1): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (sm): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 63\n",
    "output_dim = len(LABELS)\n",
    "Class_net = My_net(input_dim, output_dim)\n",
    "Class_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b76c6ed8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 iter : 0 (0m 0s) 1.1047  / Ok ✓\n",
      "epoch : 71 iter : 3 (0m 3s) 1.0403  / Victory ✓\n",
      "epoch : 142 iter : 6 (0m 6s) 0.9041  / Victory ✓\n",
      "epoch : 214 iter : 2 (0m 9s) 0.9876  / Victory ✓\n",
      "epoch : 285 iter : 5 (0m 12s) 0.9067  / Victory ✓\n",
      "epoch : 357 iter : 1 (0m 16s) 0.8665  / Victory ✓\n",
      "epoch : 428 iter : 4 (0m 19s) 0.8753  / Victory ✓\n",
      "epoch : 500 iter : 0 (0m 22s) 0.8281  / Victory ✓\n",
      "epoch : 571 iter : 3 (0m 25s) 0.7843  / Victory ✓\n",
      "epoch : 642 iter : 6 (0m 28s) 0.6321  / Victory ✓\n",
      "epoch : 714 iter : 2 (0m 31s) 0.7575  / Victory ✗ (WTF)\n",
      "epoch : 785 iter : 5 (0m 34s) 0.7575  / Victory ✓\n",
      "epoch : 857 iter : 1 (0m 37s) 0.7156  / WTF ✓\n",
      "epoch : 928 iter : 4 (0m 40s) 0.6747  / Ok ✓\n",
      "epoch : 1000 iter : 0 (0m 43s) 0.6794  / Victory ✓\n",
      "epoch : 1071 iter : 3 (0m 46s) 0.6508  / WTF ✓\n",
      "epoch : 1142 iter : 6 (0m 49s) 0.6480  / Victory ✓\n",
      "epoch : 1214 iter : 2 (0m 52s) 0.6237  / Victory ✓\n",
      "epoch : 1285 iter : 5 (0m 55s) 0.6280  / Ok ✓\n",
      "epoch : 1357 iter : 1 (0m 58s) 0.6149  / Ok ✓\n",
      "epoch : 1428 iter : 4 (1m 1s) 0.6277  / Victory ✓\n",
      "epoch : 1500 iter : 0 (1m 4s) 0.6085  / WTF ✓\n",
      "epoch : 1571 iter : 3 (1m 7s) 0.6233  / Victory ✓\n",
      "epoch : 1642 iter : 6 (1m 10s) 0.5652  / Victory ✓\n",
      "epoch : 1714 iter : 2 (1m 13s) 0.6094  / Victory ✓\n",
      "epoch : 1785 iter : 5 (1m 16s) 0.5940  / WTF ✓\n",
      "epoch : 1857 iter : 1 (1m 19s) 0.6047  / Victory ✓\n",
      "epoch : 1928 iter : 4 (1m 22s) 0.5869  / Victory ✓\n",
      "epoch : 2000 iter : 0 (1m 25s) 0.5883  / WTF ✓\n",
      "epoch : 2071 iter : 3 (1m 27s) 0.5926  / Victory ✓\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(Class_net.parameters(),lr=learning_rate,momentum=0.9)\n",
    "\n",
    "all_losses = []\n",
    "start = time.time()\n",
    "counter = 0\n",
    "for epoch in range(2100):  \n",
    "    current_loss = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output = Class_net(inputs.float())\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        category = LABELS[int(labels[0])]\n",
    "\n",
    "        if counter % 500 == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('epoch : %d iter : %d (%s) %.4f  / %s %s' % (epoch, i, timeSince(start), loss, guess, correct))\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "    if counter % 100 == 0:\n",
    "        all_losses.append(current_loss / 25)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3da10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f520bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция рисования найденных параметров на кадре\n",
    "def draw(frame, boxes, probs, landmarks):\n",
    "    try:\n",
    "        for box, prob, ld in zip(boxes, probs, landmarks):\n",
    "            # Рисуем обрамляющий прямоугольник лица на кадре\n",
    "            cv2.rectangle(frame,\n",
    "                          (int(box[0]), int(box[1])),\n",
    "                          (int(box[2]), int(box[3])),\n",
    "                          (255, 0, 255),\n",
    "                          thickness=3)\n",
    "\n",
    "            # Рисуем особенные точки\n",
    "            #cv2.circle(frame, (int(ld[0][0]),int(ld[0][1])), 5, (0, 0, 255), -1)\n",
    "            #cv2.circle(frame, (int(ld[1][0]),int(ld[1][1])), 5, (0, 0, 255), -1)\n",
    "            #cv2.circle(frame, (int(ld[2][0]),int(ld[2][1])), 5, (0, 0, 255), -1)\n",
    "            #cv2.circle(frame, (int(ld[3][0]),int(ld[3][1])), 5, (0, 0, 255), -1)\n",
    "            #cv2.circle(frame, (int(ld[4][0]),int(ld[4][1])), 5, (0, 0, 255), -1)\n",
    "    except Exception as e:\n",
    "        #print('Something wrong im draw function!')\n",
    "        #print(f'error : {e}')\n",
    "        pass\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c21fe12c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = \"./MAH04286.MP4\"\n",
    "#cap = cv2.VideoCapture(file_name)  \n",
    "\n",
    "# For webcam input:\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "gesture_text_from_session = []\n",
    "pre_char=''\n",
    "current_char=''\n",
    "Ok_count = 0\n",
    "\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.75,\n",
    "    min_tracking_confidence=0.75,\n",
    "    max_num_hands=1) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            #break\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        scale_percent = 50    \n",
    "        width  = int(image.shape[1] * scale_percent / 100)\n",
    "        height = int(image.shape[0] * scale_percent / 100)\n",
    "\n",
    "        # dsize\n",
    "        dsize = (width, height)    \n",
    "\n",
    "        # resize image\n",
    "        image = cv2.resize(image, dsize)\n",
    "        try:        \n",
    "            boxes, probs, landmarks = mtcnn.detect(image, landmarks=True)                \n",
    "            image = draw(image, boxes, probs, landmarks)        \n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "        palm_vector_list = []\n",
    "        \n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        \n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)        \n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand mesh annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for n_point in range(len(results.multi_hand_landmarks[0].landmark)):\n",
    "                        palm_vector_list.append(hand_landmarks.landmark[n_point].x)\n",
    "                        palm_vector_list.append(hand_landmarks.landmark[n_point].y)                        \n",
    "                        palm_vector_list.append(hand_landmarks.landmark[n_point].z)                \n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS) \n",
    "\n",
    "            \n",
    "            prob=torch.tensor(np.array(palm_vector_list), dtype=torch.float, device=device)\n",
    "            prob=torch.reshape(prob, (1, X_data.shape[1]))\n",
    "            \n",
    "            result = Class_net(prob)\n",
    "            \n",
    "            emotion = categoryFromOutput(result)[0]\n",
    "            \n",
    "            cv2.putText(image, \n",
    "                    emotion, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            \n",
    "            current_char = emotion\n",
    "            if current_char != pre_char:\n",
    "                gesture_text_from_session.append(current_char)\n",
    "                pre_char = current_char\n",
    "                \n",
    "                if current_char == 'Ok':\n",
    "                    Ok_count = Ok_count+1\n",
    "                    #save_obj(results.multi_hand_landmarks, './video/landmarks_palm__' + str(label) +\"___\"+ str(count) )   \n",
    "                    cv2.imwrite('video/palm_' +str(current_char)  +\"___\" + str(Ok_count) + '.png', image)\n",
    "\n",
    "                  \n",
    "            \n",
    "            cv2.imshow('MediaPipe PalmMesh', image)\n",
    "\n",
    "        else:\n",
    "            emotion = 'No hand found'\n",
    "            cv2.putText(image, \n",
    "                    emotion, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('MediaPipe PalmMesh', image)\n",
    "              \n",
    "            \n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "                break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e88a184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file=open(\"gesture_text_from_session.txt\", mode=\"w\",encoding=\"utf-8\")\n",
    "for word in gesture_text_from_session:\n",
    "    new_file.write(word +\"\\n\")    \n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef96100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
